{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries install\n",
    "!pip install transformers\n",
    "!pip install torch-scatter\n",
    "!pip install torch-sparse\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries upload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from scipy import io\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel, XLMRobertaConfig, XLMRobertaModel, XLMRobertaTokenizer\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.nn import Parameter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix, classification_report, f1_score, recall_score, precision_score, accuracy_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd57ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define device for deep learning\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"working on gpu \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"working on cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e522ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment as googledrive to folder \"resource\"\n",
    "data_path =  \"/Colab Notebooks/\"\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    data_path = \"/content/drive/MyDrive/Colab Notebooks/UCCA-CNN/\"\n",
    "\n",
    "except:\n",
    "    print(\"You are not working in Colab at the moment :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters GAT\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "batch_size = 1\n",
    "n_out = 2\n",
    "epoch_size = 50\n",
    "learning_rate = 0.0005\n",
    "init_weight_decay = 0.2\n",
    "init_clip_max_norm = 0.1\n",
    "nhid=800\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, split, feature, adj, label, length):\n",
    "        self.feature_array = np.array(feature)\n",
    "        self.adj_array = np.array(adj)\n",
    "        self.label_array = label\n",
    "        self.s_length = length\n",
    "\n",
    "        #print(\"len \", len(self.feature_array), \" \", len(self.label_array))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        selected_label = int(self.label_array[idx])\n",
    "        selected_feature = self.feature_array[idx]\n",
    "        selected_adj = self.adj_array[idx]\n",
    "        selected_length = self.s_length[idx]\n",
    "\n",
    "        return selected_feature, selected_adj, selected_label, selected_length\n",
    "\n",
    "    \n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: (x[0].shape[0]), reverse=True)\n",
    "    arrays, adjs, labels, sentence_length = zip(*data)\n",
    "    lengths = [(array.shape[0]) for array in arrays]\n",
    "    longest = max(lengths)\n",
    "    targets = np.zeros([len(arrays), max(lengths), 768])\n",
    "    targets_adj = np.zeros([len(arrays), max(lengths), max(lengths)])\n",
    "    for i, cap in enumerate(arrays):\n",
    "        end = lengths[i]\n",
    "        array = arrays[i]\n",
    "        slength = sentence_length[i]\n",
    "        adj = adjs[i]\n",
    "        new_adj = np.pad(adj, [((longest - end),0),(0,(longest - end))], mode='constant')\n",
    "        new_array = np.pad(array, [((longest - end),0),(0,0)], mode='constant')\n",
    "        targets[i,:,:] = new_array\n",
    "        targets_adj[i,:,:] = new_adj\n",
    "    return targets, targets_adj, labels, slength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.matmul(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        # Wh.shape (N, out_feature)\n",
    "        # self.a.shape (2 * out_feature, 1)\n",
    "        # Wh1&2.shape (N, 1)\n",
    "        # e.shape (N, N)\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        # broadcast add\n",
    "        e = Wh1 + Wh2.T\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "    \n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCCA_GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(UCCA_GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True).to(device) for _ in range(nheads)]\n",
    "        self.attentions1 = [GraphAttentionLayer(nhid*nheads, nhid, dropout=dropout, alpha=alpha, concat=True).to(device) for _ in range(nheads)]\n",
    "        self.attentions2 = [GraphAttentionLayer(nhid*nheads, nhid, dropout=dropout, alpha=alpha, concat=True).to(device) for _ in range(nheads)]\n",
    "        self.attentions3 = [GraphAttentionLayer(nhid*nheads, nhid, dropout=dropout, alpha=alpha, concat=True).to(device) for _ in range(nheads)]\n",
    "        self.attentions4 = [GraphAttentionLayer(nhid*nheads, nhid, dropout=dropout, alpha=alpha, concat=True).to(device) for _ in range(nheads)]\n",
    "        self.attentions5 = [GraphAttentionLayer(nhid*nheads, nhid, dropout=dropout, alpha=alpha, concat=True).to(device) for _ in range(nheads)]\n",
    "        self.attentions6 = [GraphAttentionLayer(nhid*nheads, nhid, dropout=dropout, alpha=alpha, concat=True).to(device) for _ in range(nheads)]\n",
    "\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        for i, attention in enumerate(self.attentions1):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        for i, attention in enumerate(self.attentions2):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        for i, attention in enumerate(self.attentions3):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        for i, attention in enumerate(self.attentions4):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        for i, attention in enumerate(self.attentions5):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions1], dim=1)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        x= torch.mean(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29763538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, location):\n",
    "\tfilepath = os.path.join(location, 'best.pth.tar')\n",
    "\ttorch.save(state, filepath)\n",
    "    \n",
    "def train(train_dl, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch in train_dl:\n",
    "        feature, adj, label, length = batch\n",
    "        feature, adj, label = torch.FloatTensor(feature), torch.FloatTensor(adj), torch.LongTensor(label)\n",
    "        feature, adj, label = feature.to(device), adj.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feature, adj)\n",
    "        loss = criterion(output.view(-1, n_out), label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss/float(len(train_dl))\n",
    "\n",
    "\n",
    "def evaluate(model, dl):\n",
    "    total_loss = 0\n",
    "    prediction_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\t\t\n",
    "            feature, adj, label,length = batch\n",
    "            feature, adj, label = torch.FloatTensor(feature), torch.FloatTensor(adj), torch.LongTensor(label)\n",
    "            feature, adj, label = feature.to(device), adj.to(device), label.to(device)\n",
    "            output = model(feature, adj)\n",
    "            loss = criterion(output.view(-1, n_out), label)\n",
    "            total_loss += loss.item()\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            prediction_list.extend(predicted.data.cpu().numpy())\n",
    "            label_list.extend(label.data.cpu().numpy())\n",
    "            length_list.append(length)\n",
    "    return f1_score(label_list, prediction_list, average='macro'), total_loss, label_list, prediction_list, length_list\n",
    "\n",
    "def train_and_evaluate(model, optimizer, train_dl, val_dl, test_dl=None, fold=0):\n",
    "    best_val_acc = -999.9\n",
    "    r_test_acc = -999.0\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    label_best = []\n",
    "    prediction_best = []\n",
    "    for epoch in range(1, epoch_size+1):\n",
    "        total_loss = train(train_dl, model, optimizer)\n",
    "        val_acc, val_loss, label_list, prediction_list, length_list = evaluate(model, val_dl)\n",
    "        #test_acc, test_loss = evaluate(model, test_dl)\n",
    "        #print(\"Epoch = \", epoch, \" train loss = \", total_loss, \" val_acc = \", val_acc) #, \" test_acc = \", test_acc)\n",
    "        if val_acc > best_val_acc:\n",
    "            save_checkpoint({'epoch': epoch , 'state_dict': model.state_dict(), 'optim_dict': optimizer.state_dict()}, location=data_path + 'result/')\n",
    "            best_val_acc = val_acc\n",
    "            #r_test_acc = test_acc\n",
    "            label_best = label_list\n",
    "            prediction_best = prediction_list\n",
    "        scheduler.step()\n",
    "    print(\"Best Val acc = \", best_val_acc) #, \" Test Acc = \", r_test_acc)\n",
    "    return best_val_acc,label_best, prediction_best, length_list\n",
    "\n",
    "def train_and_evaluate_fold():\n",
    "    label_all = []\n",
    "    prediction_all = []\n",
    "    best_accuracy = []\n",
    "    length_all = []\n",
    "\n",
    "    k_folds = 10\n",
    "\n",
    "    results = {}\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, random_state=RANDOM_SEED, shuffle=True)\n",
    "    # Start print\n",
    "    print('--------------------------------')\n",
    "\n",
    "    dataset = pd.DataFrame({'feature' : np.squeeze(np.array(dataset[\"feature\"])), 'adj' : np.squeeze(np.array(dataset[\"adjacency\"])), 'label' : np.squeeze(np.array(dataset[\"labels\"])), 'length' : np.squeeze(np.array(dataset[\"lenghts\"]))})\n",
    "    dataset = shuffle(dataset)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        gcn_model = UCCA_GAT(nfeat=768, \n",
    "                            nhid=nhid,\n",
    "                             nclass=n_out,\n",
    "                             dropout=dropout,\n",
    "                           alpha=0.1,\n",
    "                            nheads=1).to(device)\n",
    "        optimizer = torch.optim.Adam(gcn_model.parameters(), lr=learning_rate) #, weight_decay=init_weight_decay)\n",
    "\n",
    "        train_df = dataset.iloc[train_idx]\n",
    "\n",
    "        valid_df = dataset.iloc[val_idx]\n",
    "        print(fold)\n",
    "        dl_train = DataLoader(IronyDataset(\"train\", train_df[\"feature\"].to_numpy(), train_df[\"adj\"].to_numpy(), list(train_df[\"label\"]), list(train_df[\"length\"])), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        dl_val= DataLoader(IronyDataset(\"val\", valid_df[\"feature\"].to_numpy(), valid_df[\"adj\"].to_numpy(), list(valid_df[\"label\"]), list(valid_df[\"length\"])), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        best_a,label_list, prediction_list, length_list = train_and_evaluate(gcn_model, optimizer, dl_train, dl_val, fold)\n",
    "        best_accuracy.append(best_a)\n",
    "        label_all.extend(label_list)\n",
    "        prediction_all.extend(prediction_list)\n",
    "        length_all.extend(length_list)\n",
    "    print(np.mean(best_accuracy))\n",
    "    return label_all, prediction_all, length_all, gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "dataset = io.loadmat(data_path + 'dependency_dataset/bert_mr_xlm.mat')\n",
    "label_all, prediction_all, length_all, gcn_model = train_and_evaluate_fold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
